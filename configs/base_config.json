{
    "seed": 42,
    "train_size": 100000,
    "eval_size": 10000,
    "tokenizer_type": "unigram",
    "use_only_english": false,
    "batch_size": 1024,
    "vocab_sizes": [
        8000,
        32000
    ],
    "bert_model_config": {
      "hidden_size": 256,
      "num_hidden_layers": 4,
      "num_attention_heads": 4,
      "intermediate_size": 512,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "attention_probs_dropout_prob": 0.1,
      "max_position_embeddings": 512,
      "type_vocab_size": 2,
      "initializer_range": 0.02,
      "layer_norm_eps": 1e-12,
      "pad_token_id": 0
    },
    "training_args": {
      "learning_rate": 1e-3,
      "num_train_epochs": 10,
      "save_total_limit": 2,
      "evaluation_strategy": "steps",
      "eval_steps": 100,
      "logging_steps": 100,
      "save_steps": 300,
      "bf16": true,
      "lr_scheduler_type": "cosine",
      "warmup_steps": 300,
      "weight_decay": 0.01,   
      "max_grad_norm": 1.0,
      "batch_eval_metrics": true
},
    "block_size": 128,
    "mlm_probability": 0.15
  }